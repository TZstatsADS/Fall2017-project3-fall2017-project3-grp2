label_train <- label_train0[, -1]
# label_train[1:5,]
sift_train_data0 <- cbind(label_train, sift_train)
colnames(sift_train_data0)[1] <- "label"
# Split the sift data set into training and testing set
test_ind <- sample(1:dim(sift_train1)[1], 5, replace = F)
sift_train_data <- sift_train_data0[-test_ind, ]
sift_test_data <- sift_train_data0[test_ind, -1]
sift_test_label <- sift_train_data0[test_ind, 1]
# sift_train[1:5, 1:5]
source("randForest_yl3538.r")
```
```{r}
# Train Random Forest
hog0 <- read.csv("hog.csv")
hog1 <- hog0[, -1]
hog1$label <- sift_train_data0[,1]
system.time(randForest1 <- randForest_train(sift_train_data0, n_trees=500))
system.time(randForest_model <- randomForest(label ~ ., data = sift_train_data0, ntree=500, mtry=33, importance=T))
system.time(randForest1 <- randForest_train(hog, n_trees=500))
system.time(randForest_model <- randomForest(label ~ ., data = hog, ntree=500, mtry=5, importance=T))
# Test
#randForest_test(randForest1, sift_test_data)
#
```
```{r}
# Run Random Forest CV
#randForest_cv(train_data = sift_train_data0, n_trees_vec = c(10, 50, 100, 150), K = 5)
#randForest_cv(train_data = sift_train_data0, n_trees_vec = c(), K = 5)
```
```{r}
# Use PCA to reduce the dimension of SIFT features
# input original SIFT data
#sift.feature=read.csv("./data/sift_features.csv")
sift <- sift_train1
# add lables: 0 for dog and 1 for fried chicken
# use PCA() in feature_sift.R to reduce dimension
set.seed(500)
data <- pca_features(sift, 100)
###this label should be input by TA
# selected data with labels
pca_train_data <- cbind(data, label_train0[,2])
colnames(pca_train_data)[ncol(pca_train_data)] <- "label"
pca_train_data<-as.data.frame(pca_train_data)
write.csv(pca_train_data, "pca_train_data.csv")
setwd("~/Documents/Graduate_School_2016-2017/MA_Graduate_Classes/Applied_Data_Science_Ying_Liu/Project3_Feature_Extraction_Model_Evaluation")
# sift_train[1:5, 1:5]
source("randForest_yl3538.r")
source("pca_features.r")
# add lables: 0 for dog and 1 for fried chicken
# use PCA() in feature_sift.R to reduce dimension
set.seed(500)
data <- pca_features(sift, 100)
# add lables: 0 for dog and 1 for fried chicken
# use PCA() in feature_sift.R to reduce dimension
set.seed(500)
# selected data with labels
pca_train_data <- cbind(data, label_train0[,2])
colnames(pca_train_data)[ncol(pca_train_data)] <- "label"
pca_train_data<-as.data.frame(pca_train_data)
write.csv(pca_train_data, "pca_train_data.csv")
?gbm
baseline_cv(train_data = pca_train_data, n_trees_vec = c(10), K=3)
# Baseline Model
############ gbm ###################
train_baseline=function(train.data, n_tree)
{
gbm1=gbm(y~.,data=train.data,dist="adaboost",n.tree = n_tree,shrinkage=0.1)
n=gbm.perf(gbm1)
return(list(gbm=gbm1,n=n))
}
############ gbm end ###################
############ gbm ######################
test_baseline=function(model,test.data)
{
prediction=predict(model$gbm,test.data,n.trees=model$n)
return(prediction)
}
## Random Forest CV Start ##
baseline_cv <- function(train_data, n_trees_vec, K) {
# train_data is a matrix with the first column as "label", each row is a data point
# n_trees_vec is a vector of numbers of n_trees
# K is the number of folds
n <- nrow(train_data)
n_fold <- floor(n/K)
fold <- sample(rep(1:K, c(rep(n_fold, K-1), n-(K-1)*n_fold)))
cv_error <- matrix(NA, nrow = K, ncol = length(n_trees_vec))
running_time <- matrix(NA, nrow =  K, ncol = length(n_trees_vec))
for (i in 1:K) {
infn_train_data <- train_data[fold != i,]
infn_test_data <- train_data[fold == i,-1]
infn_test_label <- train_data[fold == i,1]
for (j in 1:length(n_trees_vec)) {
running_time[i,j] <- system.time(fit <- train_baseline(train.data = infn_train_data, n_tree = n_trees_vec[j]))[3]
pred <- test_baseline=function(model = fit,test.data = infn_test_data)
cv_error[i, j] <- mean(pred != infn_test_label)
#browser()
}
}
#cv_error
CVmisses <- apply(cv_error, 2, mean)
Running_Time <- apply(running_time, 2, mean)
data.frame(n_tree = n_trees_vec, cv_error = CVmisses, running_time = Running_Time)
}
baseline_cv(train_data = pca_train_data, n_trees_vec = c(10), K=3)
# Baseline Model
############ gbm ###################
train_baseline=function(train.data, n_tree)
{
gbm1=gbm(label~.,data=train.data,dist="adaboost",n.tree = n_tree,shrinkage=0.1)
n=gbm.perf(gbm1)
return(list(gbm=gbm1,n=n))
}
############ gbm end ###################
############ gbm ######################
test_baseline=function(model,test.data)
{
prediction=predict(model$gbm,test.data,n.trees=model$n)
return(prediction)
}
## Random Forest CV Start ##
baseline_cv <- function(train_data, n_trees_vec, K) {
# train_data is a matrix with the first column as "label", each row is a data point
# n_trees_vec is a vector of numbers of n_trees
# K is the number of folds
n <- nrow(train_data)
n_fold <- floor(n/K)
fold <- sample(rep(1:K, c(rep(n_fold, K-1), n-(K-1)*n_fold)))
cv_error <- matrix(NA, nrow = K, ncol = length(n_trees_vec))
running_time <- matrix(NA, nrow =  K, ncol = length(n_trees_vec))
for (i in 1:K) {
infn_train_data <- train_data[fold != i,]
infn_test_data <- train_data[fold == i,-1]
infn_test_label <- train_data[fold == i,1]
for (j in 1:length(n_trees_vec)) {
running_time[i,j] <- system.time(fit <- train_baseline(train.data = infn_train_data, n_tree = n_trees_vec[j]))[3]
pred <- test_baseline=function(model = fit,test.data = infn_test_data)
cv_error[i, j] <- mean(pred != infn_test_label)
#browser()
}
}
#cv_error
CVmisses <- apply(cv_error, 2, mean)
Running_Time <- apply(running_time, 2, mean)
data.frame(n_tree = n_trees_vec, cv_error = CVmisses, running_time = Running_Time)
}
baseline_cv(train_data = pca_train_data, n_trees_vec = c(10), K=3)
# Baseline Model
############ gbm ###################
train_baseline=function(train.data, n_tree)
{
gbm1=gbm(label~.,data=train.data,dist="multinomial",n.tree = n_tree,shrinkage=0.1)
n=gbm.perf(gbm1)
return(list(gbm=gbm1,n=n))
}
############ gbm end ###################
############ gbm ######################
test_baseline=function(model,test.data)
{
prediction=predict(model$gbm,test.data,n.trees=model$n)
return(prediction)
}
## Random Forest CV Start ##
baseline_cv <- function(train_data, n_trees_vec, K) {
# train_data is a matrix with the first column as "label", each row is a data point
# n_trees_vec is a vector of numbers of n_trees
# K is the number of folds
n <- nrow(train_data)
n_fold <- floor(n/K)
fold <- sample(rep(1:K, c(rep(n_fold, K-1), n-(K-1)*n_fold)))
cv_error <- matrix(NA, nrow = K, ncol = length(n_trees_vec))
running_time <- matrix(NA, nrow =  K, ncol = length(n_trees_vec))
for (i in 1:K) {
infn_train_data <- train_data[fold != i,]
infn_test_data <- train_data[fold == i,-1]
infn_test_label <- train_data[fold == i,1]
for (j in 1:length(n_trees_vec)) {
running_time[i,j] <- system.time(fit <- train_baseline(train.data = infn_train_data, n_tree = n_trees_vec[j]))[3]
pred <- test_baseline=function(model = fit,test.data = infn_test_data)
cv_error[i, j] <- mean(pred != infn_test_label)
#browser()
}
}
#cv_error
CVmisses <- apply(cv_error, 2, mean)
Running_Time <- apply(running_time, 2, mean)
data.frame(n_tree = n_trees_vec, cv_error = CVmisses, running_time = Running_Time)
}
baseline_cv(train_data = pca_train_data, n_trees_vec = c(10), K=3)
# Baseline Model
############ gbm ###################
train_baseline=function(train.data, n_tree)
{
gbm1=gbm(label~.,data=train.data,dist="multinomial",n.tree = n_tree,shrinkage=0.1)
n=gbm.perf(gbm1)
return(list(gbm=gbm1,n=n))
}
############ gbm end ###################
############ gbm ######################
test_baseline=function(model,test.data)
{
prediction=predict(model$gbm,test.data,n.trees=model$n)
return(prediction)
}
## Random Forest CV Start ##
baseline_cv <- function(train_data, n_trees_vec, K) {
# train_data is a matrix with the first column as "label", each row is a data point
# n_trees_vec is a vector of numbers of n_trees
# K is the number of folds
n <- nrow(train_data)
n_fold <- floor(n/K)
fold <- sample(rep(1:K, c(rep(n_fold, K-1), n-(K-1)*n_fold)))
cv_error <- matrix(NA, nrow = K, ncol = length(n_trees_vec))
running_time <- matrix(NA, nrow =  K, ncol = length(n_trees_vec))
for (i in 1:K) {
infn_train_data <- train_data[fold != i,]
infn_test_data <- train_data[fold == i,-1]
infn_test_label <- train_data[fold == i,1]
for (j in 1:length(n_trees_vec)) {
running_time[i,j] <- system.time(fit <- train_baseline(train.data = infn_train_data, n_tree = n_trees_vec[j]))[3]
pred <- test_baseline(model = fit,test.data = infn_test_data)
cv_error[i, j] <- mean(pred != infn_test_label)
#browser()
}
}
#cv_error
CVmisses <- apply(cv_error, 2, mean)
Running_Time <- apply(running_time, 2, mean)
data.frame(n_tree = n_trees_vec, cv_error = CVmisses, running_time = Running_Time)
}
baseline_cv(train_data = pca_train_data, n_trees_vec = c(10), K=3)
# Baseline Model
############ gbm ###################
train_baseline=function(train.data, n_tree)
{
gbm1=gbm(label~.,data=train.data,dist="multinomial",n.tree = n_tree,shrinkage=0.1)
n=gbm.perf(gbm1)
return(list(gbm=gbm1,n=n))
}
############ gbm end ###################
############ gbm ######################
test_baseline=function(model,test.data)
{
prediction=predict(model$gbm,test.data,n.trees=model$n)
return(prediction)
}
## Random Forest CV Start ##
baseline_cv <- function(train_data, n_trees_vec, K) {
# train_data is a matrix with the first column as "label", each row is a data point
# n_trees_vec is a vector of numbers of n_trees
# K is the number of folds
n <- nrow(train_data)
n_fold <- floor(n/K)
fold <- sample(rep(1:K, c(rep(n_fold, K-1), n-(K-1)*n_fold)))
cv_error <- matrix(NA, nrow = K, ncol = length(n_trees_vec))
running_time <- matrix(NA, nrow =  K, ncol = length(n_trees_vec))
for (i in 1:K) {
infn_train_data <- train_data[fold != i,]
infn_test_data <- train_data[fold == i,-1]
infn_test_label <- train_data[fold == i,1]
for (j in 1:length(n_trees_vec)) {
running_time[i,j] <- system.time(fit <- train_baseline(train.data = infn_train_data, n_tree = n_trees_vec[j]))[3]
pred <- test_baseline(model = fit,test.data = infn_test_data)
cv_error[i, j] <- mean(pred != infn_test_label)
browser()
}
}
#cv_error
CVmisses <- apply(cv_error, 2, mean)
Running_Time <- apply(running_time, 2, mean)
data.frame(n_tree = n_trees_vec, cv_error = CVmisses, running_time = Running_Time)
}
baseline_cv(train_data = pca_train_data, n_trees_vec = c(10), K=3)
# Baseline Model
############ gbm ###################
train_baseline=function(train.data, n_tree)
{
gbm1=gbm(label~.,data=train.data,dist="multinomial",n.tree = n_tree,shrinkage=0.1)
n=gbm.perf(gbm1)
return(list(gbm=gbm1,n=n))
}
############ gbm end ###################
############ gbm ######################
test_baseline=function(model,test.data)
{
prediction=predict(model$gbm,test.data,n.trees=model$n)
return(prediction)
}
## Random Forest CV Start ##
baseline_cv <- function(train_data, n_trees_vec, K) {
# train_data is a matrix with the first column as "label", each row is a data point
# n_trees_vec is a vector of numbers of n_trees
# K is the number of folds
n <- nrow(train_data)
n_fold <- floor(n/K)
fold <- sample(rep(1:K, c(rep(n_fold, K-1), n-(K-1)*n_fold)))
cv_error <- matrix(NA, nrow = K, ncol = length(n_trees_vec))
running_time <- matrix(NA, nrow =  K, ncol = length(n_trees_vec))
for (i in 1:K) {
infn_train_data <- train_data[fold != i,]
infn_test_data <- train_data[fold == i,-1]
infn_test_label <- train_data[fold == i,1]
for (j in 1:length(n_trees_vec)) {
browser()
running_time[i,j] <- system.time(fit <- train_baseline(train.data = infn_train_data, n_tree = n_trees_vec[j]))[3]
pred <- test_baseline(model = fit,test.data = infn_test_data)
cv_error[i, j] <- mean(pred != infn_test_label)
}
}
#cv_error
CVmisses <- apply(cv_error, 2, mean)
Running_Time <- apply(running_time, 2, mean)
data.frame(n_tree = n_trees_vec, cv_error = CVmisses, running_time = Running_Time)
}
baseline_cv(train_data = pca_train_data, n_trees_vec = c(10), K=3)
running_time[i,j] <- system.time(fit <- train_baseline(train.data = infn_train_data, n_tree = n_trees_vec[j]))[3]
train_baseline=function(train.data, n_tree)
{
gbm1=gbm(label~.,data=train.data,n.tree = n_tree,shrinkage=0.1)
n=gbm.perf(gbm1)
return(list(gbm=gbm1,n=n))
}
running_time[i,j] <- system.time(fit <- train_baseline(train.data = infn_train_data, n_tree = n_trees_vec[j]))[3]
running_time[i,j] <- system.time(fit <- train_baseline(train.data = infn_train_data, n_tree = n_trees_vec[j]))[3]
train_baseline=function(train.data)
{
gbm1=gbm(label~.,data=train.data,dist="multinomial",n.tree = 400,shrinkage=0.1)
n=gbm.perf(gbm1)
return(list(gbm=gbm1,n=n))
}
base_model1 <- train_baseline(pca_train_data)
mean(test_baseline(base_model1,pca_train_data) != abel_train0[,2])
mean(test_baseline(base_model1,pca_train_data) != label_train0[,2])
base_pred2 <- test_baseline(base_model1,pca_train_data)
mean(base_pred2 != label_train0[,2])
head(base_pred2)
floor(base_pred2)
dim(base_pred2)
head(base_pred2)
base_pred2 <- test_baseline(base_model1,pca_train_data[-1])
base_pred2 <- test_baseline(base_model1,pca_train_data[-1,])
mean(base_pred2 != label_train0[,2])
dim(base_pred2)
base_pred2 <- test_baseline(base_model1,pca_train_data[,-1])
mean(base_pred2 != label_train0[,2])
dim(base_pred2)
base_pred2 <- test_baseline(base_model1,pca_train_data[,-1])
dim(base_pred2)
pca_train_data[1:5,1:5]
pca_train_data[1:5,ncol(pca_train_data)]
base_pred2 <- test_baseline(base_model1, pca_train_data[, -ncol(pca_train_data)])
mean(base_pred2 != label_train0[,2])
dim(base_pred2)
mean(base_pred2[,1] != label_train0[,2])
mean(base_pred2[,1,] != label_train0[,2])
base_pred2[,1,]
mean(floor(base_pred2[,1,]) != label_train0[,2])
mean(floor(base_pred2[,2,]) == label_train0[,2])
mean(floor(base_pred2[,3,]) == label_train0[,2])
mean(floor(base_pred2[,1,]) == label_train0[,2])
mean(floor(base_pred2[,2,]) == label_train0[,2])
source("../lib/pca_features.r")
setwd("~/Documents/Graduate_School_2016-2017/MA_Graduate_Classes/Applied_Data_Science_Ying_Liu/Project3_Feature_Extraction_Model_Evaluation")
source("../lib/pca_features.r")
source("../lib/pca_features.r")
setwd("~/Downloads/Fall2017-project3-fall2017-project3-grp2/doc")
packages.used=c("gbm", "caret","DMwR" ,"nnet","randomForest","EBImage","e1071")
source("../lib/eco2121.train_gbm_baseline.R")
source("../lib/pca_features.r")
source("../lib/xgboost+sift+pca100.R")
source("../lib/eco2121.train_gbm_baseline.r")
source("../lib/eco2121_train_gbm_baseline.R")
setwd("~/Downloads/Fall2017-project3-fall2017-project3-grp2/doc")
source("../lib/eco2121_train_gbm_baseline.R")
source("../lib/eco2121_train_gbm_baseline.r")
source("../lib/pca_features.r")
source("../lib/eco2121_train_gbm_baseline.r")
source("../lib/xgboost_sift_pca100.r")
setwd("~/Downloads/Fall2017-project3-fall2017-project3-grp2/doc")
source("../lib/eco2121_train_gbm_baseline.r")
source("../lib/pca_features.r")
source("../lib/xgboost_sift_pca100.r")
source("../lib/eco2121_train_gbm_baseline.r")
source("../lib/pca_features.r")
source("../lib/xgboost_sift_pca100.r")
source("../lib/xgboost_sift_pca100.r")
source("../lib/new_xgboost_sift_pca100.r")
source("../lib/new_xgboost_sift_pca100.r")
source("../lib/new_xgboost_sift_pca100.r")
source("../lib/new_xgboost_sift_pca100.r")
sift_train0 <- read.csv("./data/sift_train.csv", header=F)
label_train0 <- read.csv("./data/label_train.csv", header=F)
sift_train0 <- read.csv("./data/sift_train.csv", header=F)
setwd("~/Downloads/Fall2017-project3-fall2017-project3-grp2/doc")
sift_train0 <- read.csv("./data/sift_train.csv", header=F)
sift_train0 <- read.csv("../data/sift_train.csv", header=F)
label_train0 <- read.csv("../data/label_train.csv", header=F)
source("../lib/eco2121_train_gbm_baseline.r")
source("../lib/pca_features.r")
sift_pca<-read.csv("feature_pca100.csv",header = T, as.is = T)
sift_pca<-read.csv("../data/feature_pca100.csv",header = T, as.is = T)
label<-read.csv("label_train.csv",header = T,as.is = T)
dat<-cbind(label[,2],sift_pca[,-1])
label<-read.csv("../data/label_train.csv",header = T,as.is = T)
dat<-cbind(label[,2],sift_pca[,-1])
colnames(dat)[1]<-"label"
# Train and test split
train_index<-sample(1:nrow(dat),0.7*nrow(dat))
xgb_variables<-as.matrix(dat[,-1]) # Full dataset
xgb_label<-dat[,1] # Full label
# Split train data
xgb_train<-xgb_variables[train_index,]
train_label<-xgb_label[train_index]
train_matrix<-xgb.DMatrix(data = xgb_train, label=train_label)
# Split test data
xgb_test<-xgb_variables[-train_index,]
test_label<-xgb_label[-train_index]
source("../lib/eco2121_train_gbm_baseline.r")
packages.used=c("gbm", "caret","DMwR" ,"nnet","randomForest","EBImage","e1071","xgboost")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
train_matrix<-xgb.DMatrix(data = xgb_train, label=train_label)
??xgb.DMatrix
source("../lib/eco2121_train_gbm_baseline.r")
train_matrix<-xgb.DMatrix(data = xgb_train, label=train_label)
# Split test data
xgb_test<-xgb_variables[-train_index,]
test_label<-xgb_label[-train_index]
test_matrix<-xgb.DMatrix(data = xgb_test, label=test_label)
# Basic model
basic = xgboost(data = train_matrix,
max.depth=3,eta=0.01,nthread=2,nround=50,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 3,
verbose = F)
# Basic model
basic = xgboost(data = train_matrix,
max.depth=3,eta=0.01,nthread=2,nround=50,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 3,
verbose = F)
# Tune the model
xgb_params_3 = list(objective="multi:softprob",
eta = 0.01,
max.depth = 3,
eval_metric = "mlogloss",
num_class = 3)
# fit the model with arbitrary parameters
xgb_3 = xgboost(data = train_matrix,
params = xgb_params_3,
nrounds = 100,
verbose = F)
# cross validation
xgb_cv_3 = xgb.cv(params = xgb_params_3,
data = train_matrix,
nrounds = 100,
nfold = 5,
showsd = T,
stratified = T,
verbose = F,
prediction = T)
# set up the cross validated hyper-parameter search
xgb_grid_3 = expand.grid(nrounds=c(100,250,500),
eta = c(1,0.1,0.01),
max_depth = c(2,4,6,8,10),
gamma=1,
colsample_bytree=0.5,
min_child_weight=2,
subsample = 1)
# pack the training control parameters
xgb_trcontrol_3 = trainControl(method = "cv",
number = 5,
verboseIter = T,
returnData = F,
returnResamp = "all",
allowParallel = T)
# train the model for each parameter combination in the grid
ptm <- proc.time() ## start the time
xgb_train_3 = train(x=train_matrix, y=train_label,
trControl = xgb_trcontrol_3,
tuneGrid = xgb_grid_3,
method = "xgbTree")
sift_train = read_csv("./data/sift_train.csv")
label = read_csv("./data/label_train.csv")
data = data.frame(label[,2], sift_train[,2:ncol(sift_train)])
colnames(data)[1] = "label"
system.time(result<-gbm_train(train_data[,2:ncol(train_data)],train_data$label))
index = sample(1:nrow(data), size=0.7*nrow(data))
train_data = data[index,]
test_data = data[-index,]
# dat_train = training features
# label_train = labels
# K = number of folds
# d = a certain interaction depth
system.time(result<-gbm_train(train_data[,2:ncol(train_data)],train_data$label))
sift_train = read_csv("../data/sift_train.csv")
sift_train = read.csv("../data/sift_train.csv")
label = read.csv("../data/label_train.csv")
data = data.frame(label[,2], sift_train[,2:ncol(sift_train)])
colnames(data)[1] = "label"
set.seed(123)
index = sample(1:nrow(data), size=0.7*nrow(data))
train_data = data[index,]
test_data = data[-index,]
